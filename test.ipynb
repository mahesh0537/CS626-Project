{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/pytorch-pretrained-bert/#usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmphablk9_e\n",
      "100%|███████████████████████████| 231508/231508 [00:04<00:00, 50458.26B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmphablk9_e to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmphablk9_e\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] Who was jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpzw3jeykh\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_b = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model_b.to('cuda')\n",
    "with torch.no_grad():\n",
    "    predictions = model_b(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8394e-01, -1.7001e-01,  7.6305e-01,  2.7892e-01, -4.8184e-01,\n",
       "         -1.5493e-01,  7.3269e-01,  1.6251e-01,  6.1796e-01, -9.9670e-01,\n",
       "          2.8407e-01, -3.8097e-01,  9.6230e-01, -5.9843e-01,  8.1381e-01,\n",
       "         -2.8998e-01, -2.1117e-02, -3.7284e-01,  2.4951e-01, -5.1159e-01,\n",
       "          3.4482e-01, -5.2108e-02,  6.9402e-01,  1.7448e-01,  2.1442e-01,\n",
       "         -6.5071e-01, -3.9092e-01,  8.3451e-01,  8.8650e-01,  6.0891e-01,\n",
       "         -4.4702e-01,  1.5718e-01, -9.5943e-01, -1.0288e-01,  7.5984e-01,\n",
       "         -9.4224e-01, -9.6534e-03, -6.1188e-01,  6.4898e-02, -2.2315e-02,\n",
       "         -7.4684e-01,  1.9709e-01,  9.2341e-01, -6.7321e-01, -2.7453e-01,\n",
       "         -3.0136e-01, -9.6742e-01,  1.3600e-01, -7.3310e-01, -7.5566e-01,\n",
       "         -6.1855e-01, -8.1416e-01,  7.8206e-02,  1.7842e-01,  2.7548e-01,\n",
       "          4.7248e-01, -1.7435e-01,  9.7999e-02, -3.9871e-02, -4.0963e-01,\n",
       "         -4.9912e-01,  1.0898e-01,  5.8327e-01, -7.7384e-01, -5.9771e-01,\n",
       "         -7.8410e-01, -3.9068e-02, -1.0386e-01,  2.7296e-02, -6.9468e-03,\n",
       "          7.0008e-01,  1.2624e-01,  3.8297e-01, -6.0384e-01, -6.7071e-01,\n",
       "          1.2134e-01, -1.7204e-01,  9.9563e-01, -1.1894e-02, -9.4180e-01,\n",
       "         -6.7445e-01, -5.1462e-01,  8.6164e-02,  7.4799e-01, -7.3040e-01,\n",
       "         -9.8547e-01,  1.4601e-01, -4.2803e-02, -9.6745e-01,  1.2629e-01,\n",
       "          6.6125e-02, -1.1508e-01, -6.8819e-01,  1.0266e-01,  4.6183e-02,\n",
       "          9.3832e-02, -8.2508e-02,  5.9638e-01, -3.8201e-03,  2.0776e-01,\n",
       "         -8.1104e-02, -4.9678e-02,  1.4098e-01, -1.9628e-01, -6.6251e-03,\n",
       "         -2.0630e-01, -1.1790e-01, -5.0903e-02, -3.8836e-01,  4.4600e-01,\n",
       "          1.1235e-01, -8.4555e-02,  7.3477e-02, -8.9191e-01,  4.2789e-01,\n",
       "         -1.1400e-01, -9.4305e-01, -1.4402e-01, -9.6397e-01,  4.7462e-01,\n",
       "          1.6938e-01, -9.9285e-02,  8.9398e-01,  7.5003e-01,  1.4137e-01,\n",
       "          5.8279e-02,  7.6956e-01, -9.9763e-01,  1.7324e-01,  1.5488e-01,\n",
       "          4.3675e-01,  7.2030e-03, -9.3345e-01, -8.9547e-01,  3.7925e-01,\n",
       "          8.8132e-01,  6.6369e-02,  9.2265e-01, -2.0229e-02,  8.4170e-01,\n",
       "          4.8328e-01,  9.1467e-02, -6.1105e-01, -2.9682e-01, -6.5243e-02,\n",
       "          1.0881e-01, -3.9002e-01,  1.9304e-01,  3.6784e-01, -4.8809e-01,\n",
       "          1.9329e-01, -1.5462e-01,  6.4554e-01, -8.3649e-01, -3.1580e-01,\n",
       "          8.1280e-01,  4.6286e-01,  7.6771e-01,  7.4494e-01, -6.9802e-02,\n",
       "         -1.6733e-01,  6.5681e-01, -3.4390e-03,  1.8204e-01,  1.3786e-01,\n",
       "          2.0551e-01, -4.4176e-01,  2.2828e-01, -7.1451e-01,  4.3156e-01,\n",
       "          2.3258e-01, -4.0639e-02,  7.5003e-01, -9.3604e-01, -1.7879e-01,\n",
       "          1.9476e-01,  9.5604e-01,  6.1967e-01,  1.0477e-01, -3.8727e-01,\n",
       "         -7.0600e-02, -2.2288e-01, -8.4797e-01,  9.3012e-01, -2.4859e-02,\n",
       "          1.4594e-01,  6.5212e-01, -3.2320e-01, -7.0216e-01, -6.1566e-01,\n",
       "          6.6355e-01,  3.2644e-01, -6.8589e-01,  1.4731e-01, -3.2455e-01,\n",
       "         -2.1601e-01,  5.4786e-01,  2.6916e-01, -1.4821e-01, -2.7069e-01,\n",
       "          1.1953e-01,  8.2587e-01,  8.2905e-01,  5.5835e-01, -6.4786e-01,\n",
       "          2.3292e-01, -7.8149e-01, -1.3565e-01,  1.0392e-01,  1.7761e-01,\n",
       "         -3.9505e-02,  9.7392e-01,  2.5417e-01, -5.3494e-02, -7.9187e-01,\n",
       "         -9.5063e-01,  4.9540e-02, -7.5530e-01,  5.4437e-02, -4.4637e-01,\n",
       "         -3.6961e-02,  6.9685e-01, -4.7412e-01,  1.4502e-01, -8.7458e-01,\n",
       "         -6.2553e-01,  1.6271e-01, -1.1813e-01,  2.4252e-01, -1.1755e-01,\n",
       "          1.4723e-01, -6.8558e-01, -3.5997e-01,  5.9255e-01,  7.8072e-01,\n",
       "          7.7329e-01, -5.1440e-01,  5.7058e-01, -1.1858e-01,  7.4656e-01,\n",
       "         -3.3747e-01,  8.5935e-01, -5.7287e-01,  8.3485e-02, -8.2329e-01,\n",
       "          5.5937e-01, -7.6748e-01,  5.4661e-01, -1.4145e-02, -7.6954e-01,\n",
       "         -5.7431e-01,  1.2993e-01,  1.1306e-01,  8.3526e-01, -2.2050e-01,\n",
       "          9.4995e-01, -3.8990e-01, -8.9079e-01,  2.0196e-01,  1.7718e-01,\n",
       "         -9.5843e-01, -5.9858e-01,  1.3693e-01, -7.5605e-01, -2.0253e-01,\n",
       "         -1.5947e-01, -8.8260e-01,  6.1572e-01,  1.1650e-01,  9.1006e-01,\n",
       "          2.7148e-01, -7.2653e-01,  4.6181e-02, -8.0300e-01, -1.7733e-01,\n",
       "          3.7917e-02,  7.5516e-01, -1.2350e-01, -8.9777e-01,  3.1482e-01,\n",
       "          3.6220e-01,  2.4547e-01,  7.7702e-01,  9.6303e-01,  8.6859e-01,\n",
       "          9.3482e-01,  7.8184e-01,  6.3901e-01,  1.3674e-01,  1.3308e-01,\n",
       "          9.9804e-01,  4.1273e-01, -9.7343e-01, -8.7905e-01, -3.1686e-01,\n",
       "          2.1986e-01, -9.9691e-01, -1.0835e-01,  3.7394e-02, -8.3269e-01,\n",
       "         -6.1759e-01,  9.3997e-01,  9.0328e-01, -9.9228e-01,  7.2132e-01,\n",
       "          8.2242e-01, -1.9134e-01, -4.8203e-01,  6.8878e-02,  9.4144e-01,\n",
       "          1.7643e-01,  3.0544e-01, -3.3771e-02,  2.0508e-01,  4.3602e-01,\n",
       "         -6.5913e-01,  6.1327e-01,  5.7946e-01, -5.3692e-01,  2.2468e-02,\n",
       "         -3.8562e-01, -8.2628e-01, -4.0356e-01, -6.2136e-02, -3.8848e-01,\n",
       "         -8.8411e-01, -3.2288e-02, -5.4743e-01,  2.4339e-01, -1.3898e-02,\n",
       "          3.7474e-02, -6.0044e-01,  8.2008e-02, -7.8052e-01,  1.9464e-01,\n",
       "          2.7739e-01, -8.3227e-01, -2.8603e-01,  2.1869e-01, -4.9550e-01,\n",
       "          5.6972e-01, -8.8353e-01,  9.1650e-01, -1.0196e-01, -6.1473e-01,\n",
       "          9.9461e-01, -2.6405e-01, -6.9697e-01, -3.0732e-02, -9.4099e-03,\n",
       "          1.9365e-01,  9.9146e-01, -1.0207e-01, -9.4087e-01, -1.1921e-01,\n",
       "         -1.7566e-01, -1.5271e-01, -1.7947e-02,  9.7824e-01, -2.0386e-02,\n",
       "          5.3483e-01,  5.9404e-01,  9.3047e-01, -9.6393e-01, -5.7818e-01,\n",
       "         -7.5749e-01, -8.8827e-01,  9.0948e-01,  8.5808e-01, -1.5121e-02,\n",
       "         -1.8824e-01,  9.2266e-03,  4.9449e-01,  8.0664e-02, -8.7677e-01,\n",
       "          3.2070e-01,  2.9616e-01, -6.5179e-02,  7.9012e-01, -6.2861e-01,\n",
       "         -1.1922e-01,  2.5936e-01,  2.5157e-01,  4.1926e-01, -7.2700e-01,\n",
       "          2.4504e-01, -3.4512e-02, -1.4207e-02, -1.0757e-01,  1.2335e-01,\n",
       "         -9.1603e-01, -3.7104e-01,  9.8883e-01,  2.2430e-01, -7.1403e-01,\n",
       "          5.4308e-02, -4.8989e-02, -3.8147e-01,  1.6037e-01,  2.0584e-01,\n",
       "         -1.2336e-01, -6.0545e-01, -5.5406e-01, -7.4556e-01, -9.5946e-01,\n",
       "          4.3442e-01,  9.6538e-02, -1.0610e-01,  8.9252e-01,  9.8087e-02,\n",
       "          9.8799e-03, -2.5474e-01, -5.4496e-01, -1.5785e-02,  3.0263e-01,\n",
       "         -7.6484e-01,  9.1569e-01, -1.6638e-01,  1.0098e-01,  5.8289e-01,\n",
       "          6.6546e-01, -1.6134e-01, -3.9155e-01, -5.0387e-02, -8.0011e-01,\n",
       "          1.0503e-01, -8.6324e-01,  8.8380e-01, -7.3799e-01,  1.4511e-01,\n",
       "         -1.1175e-02, -4.9883e-01,  9.9249e-01,  1.7848e-01,  3.8084e-01,\n",
       "         -4.0176e-01,  6.4609e-01,  1.0379e-01, -4.3819e-01, -2.5399e-01,\n",
       "          3.0583e-02,  7.3678e-01, -6.7774e-02,  9.7046e-02, -9.1840e-01,\n",
       "         -6.8340e-01, -5.7085e-01, -8.6310e-01, -9.6834e-01,  6.9321e-01,\n",
       "          5.0306e-01,  5.0850e-02,  4.2078e-01, -3.7143e-01, -4.1057e-01,\n",
       "         -9.9244e-02,  4.4631e-02, -8.7035e-01,  7.3967e-01, -1.4552e-01,\n",
       "          2.1974e-01, -1.6409e-01,  1.9130e-01, -7.7499e-01,  8.3421e-01,\n",
       "          7.5748e-01,  2.4507e-01, -4.7272e-02, -6.0968e-01,  5.2671e-01,\n",
       "         -4.9397e-01,  7.8000e-01, -1.0478e-01,  9.9572e-01, -1.9361e-01,\n",
       "         -6.0211e-01,  4.9982e-01,  4.4524e-01,  2.8949e-02,  7.6945e-02,\n",
       "         -7.5188e-01,  3.9107e-02,  6.7801e-01,  7.3808e-01, -6.2151e-01,\n",
       "         -7.8778e-02,  1.6228e-01, -6.1583e-01, -7.2337e-01,  5.7840e-01,\n",
       "         -2.4749e-01,  3.4108e-02,  1.5928e-02,  3.6058e-02,  9.7327e-01,\n",
       "         -3.6508e-03,  9.3323e-02, -2.4411e-01,  9.2947e-02, -1.8078e-01,\n",
       "         -3.3199e-01,  9.7735e-01,  2.2661e-01, -3.6291e-01, -9.6681e-01,\n",
       "          6.2984e-01, -7.5599e-01,  7.2867e-01,  6.7014e-01, -6.8131e-01,\n",
       "          1.6349e-01,  3.5664e-02, -1.7307e-01,  4.3261e-01, -6.5748e-02,\n",
       "         -1.5617e-01,  5.7030e-02,  9.5222e-02,  9.1078e-01, -2.5796e-01,\n",
       "         -9.1123e-01, -4.1688e-01,  9.1688e-02, -8.8378e-01, -1.4639e-01,\n",
       "         -2.7787e-01, -6.2363e-02, -2.0926e-01,  5.6321e-01,  5.7742e-01,\n",
       "         -1.2915e-01, -9.3684e-01, -4.1591e-02, -8.2009e-02,  9.0999e-01,\n",
       "          4.4430e-02, -1.3499e-01, -7.6171e-01, -7.0686e-01, -3.6960e-01,\n",
       "          7.4846e-01, -8.6284e-01,  9.2693e-01, -9.1248e-01,  4.5784e-02,\n",
       "          9.8189e-01,  2.6636e-01, -7.5438e-01,  5.4461e-02, -1.8298e-01,\n",
       "          1.1413e-01,  2.9297e-01,  3.1917e-01, -8.8165e-01, -1.0923e-01,\n",
       "         -4.2715e-02,  1.5434e-01, -3.5807e-02,  4.5589e-01,  4.8163e-01,\n",
       "          9.7897e-02, -9.8660e-02, -3.5813e-01,  1.9364e-02,  2.5354e-01,\n",
       "          4.1076e-01, -1.8258e-01, -2.5312e-02,  6.5899e-02, -5.4738e-02,\n",
       "         -7.4540e-01, -1.3688e-01,  3.9801e-02, -2.9774e-01,  4.2485e-01,\n",
       "         -9.9513e-01, -6.5527e-01, -7.1079e-01, -1.3351e-01,  6.7027e-01,\n",
       "         -5.2795e-02, -3.3853e-01, -5.0394e-01,  6.8103e-01,  8.2652e-01,\n",
       "          4.9607e-01,  1.6355e-02,  5.4430e-01, -4.4894e-01, -5.2525e-02,\n",
       "         -9.4828e-04,  1.1357e-01,  5.3347e-01,  6.0704e-01, -4.8155e-02,\n",
       "          9.9691e-01,  1.7057e-02, -6.9616e-02, -8.5016e-01,  1.5631e-01,\n",
       "         -9.0704e-02,  9.1693e-01, -7.0686e-01, -8.8581e-01,  6.0666e-02,\n",
       "         -1.1010e-01, -6.1249e-01,  6.6564e-02, -1.8065e-02, -3.4643e-01,\n",
       "          5.7162e-01,  8.4475e-01,  7.0373e-01, -1.3911e-01,  1.2414e-01,\n",
       "         -1.7735e-01, -1.1062e-01, -5.2390e-02, -7.4563e-01,  9.6395e-01,\n",
       "          1.7240e-01,  6.9283e-01,  6.1433e-01,  2.0058e-01,  9.0821e-01,\n",
       "          5.2692e-02,  3.9649e-01, -2.2988e-02,  9.7974e-01,  1.1133e-01,\n",
       "         -8.0435e-01,  5.1525e-01, -9.5229e-01, -7.5949e-02, -8.6190e-01,\n",
       "          1.5781e-01, -1.3121e-01,  7.0884e-01, -1.4262e-01,  8.8478e-01,\n",
       "          8.0770e-01, -8.5402e-03,  4.0688e-01,  7.8783e-01,  2.4241e-01,\n",
       "         -7.8761e-01, -9.5943e-01, -9.6765e-01, -3.4516e-02, -2.6013e-01,\n",
       "         -1.8287e-02,  1.9245e-01,  8.6139e-02,  1.7237e-01,  1.4415e-01,\n",
       "         -9.7071e-01,  8.4682e-01,  1.7703e-01, -7.0809e-01,  9.0388e-01,\n",
       "         -1.9840e-01,  1.7801e-02,  1.7389e-01, -9.5987e-01, -8.1490e-01,\n",
       "         -1.6985e-01, -2.0695e-01,  4.5600e-01,  2.8402e-01,  7.0571e-01,\n",
       "          1.0909e-01, -4.0231e-01,  2.6220e-03,  6.9869e-01, -2.7132e-01,\n",
       "         -9.7347e-01,  2.7398e-01,  5.5604e-01, -8.0907e-01,  8.7701e-01,\n",
       "         -5.4523e-01, -1.1757e-01,  7.2469e-01,  5.9756e-01,  7.5568e-01,\n",
       "          4.3112e-01,  2.9568e-01,  1.2372e-01,  3.8775e-01,  7.5827e-01,\n",
       "          8.1705e-01,  9.6365e-01,  5.8654e-01,  4.7142e-01,  6.6036e-01,\n",
       "          1.3774e-01,  3.7298e-01, -8.5861e-01, -7.5437e-04, -3.2988e-01,\n",
       "          1.7623e-01,  1.2213e-01, -8.1589e-02, -8.3641e-01,  2.1978e-01,\n",
       "         -3.1886e-03,  2.6100e-01, -1.4334e-01,  2.2755e-01, -2.7324e-01,\n",
       "         -3.6871e-02, -5.3918e-01, -9.5253e-02,  2.4627e-01,  1.0961e-01,\n",
       "          8.3554e-01, -2.2075e-01, -1.3258e-02, -1.7775e-01,  4.3492e-03,\n",
       "          6.7537e-01, -8.5345e-01,  7.4986e-01,  8.2422e-02,  7.2521e-01,\n",
       "         -5.5433e-01, -1.6391e-01,  4.3568e-01, -4.1666e-01, -2.0105e-01,\n",
       "         -1.8884e-01, -5.8743e-01,  5.7309e-01,  4.6720e-02, -2.4201e-01,\n",
       "         -1.9976e-01,  3.6003e-01,  1.6275e-01,  1.4168e-01,  4.9029e-01,\n",
       "          6.0909e-01,  1.2642e-01, -4.3185e-02,  1.6119e-01,  3.8496e-02,\n",
       "         -9.7302e-01,  2.3081e-01,  5.3640e-01, -5.5669e-01,  4.2383e-01,\n",
       "         -6.1581e-01,  3.8237e-01, -8.9079e-01,  1.5224e-02, -2.7554e-01,\n",
       "         -5.4432e-01, -4.3084e-01,  9.9598e-02,  9.1808e-02,  7.0052e-01,\n",
       "         -6.2005e-01,  7.3189e-01,  3.5388e-01,  6.6954e-01,  1.5833e-01,\n",
       "          4.8674e-01, -4.5825e-01,  7.5532e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
